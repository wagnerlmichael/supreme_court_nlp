{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import subprocess as sp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "!pip install datasets\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = sp.getoutput('git rev-parse --show-toplevel')\n",
    "os.chdir(base_path)\n",
    "\n",
    "from src import embed, pred_models, model_helpers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of files in folder\n",
    "folder_path = \"data/\"\n",
    "file_list = os.listdir(folder_path)\n",
    "\n",
    "# Create empty list to hold dataframes\n",
    "df_list = []\n",
    "\n",
    "# Loop through files in folder\n",
    "for file in file_list:\n",
    "    # Check if file is a CSV\n",
    "    if file.endswith('.csv'):\n",
    "        # Read CSV file into a pandas dataframe\n",
    "        df = pd.read_csv(os.path.join(folder_path, file))\n",
    "        # Append dataframe to list\n",
    "        df_list.append(df)\n",
    "\n",
    "# Concatenate all dataframes in list into a single dataframe\n",
    "df = pd.concat(df_list, axis=0, ignore_index=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select Top Justices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get judges columns\n",
    "j_columns = [col for col in df.columns if col.startswith('votes_side_j_')]\n",
    "\n",
    "count = 0\n",
    "justices_list = []\n",
    "justices_dict = {}\n",
    "\n",
    "for justice in j_columns:\n",
    "  df_j = df[['case_id', 'text'] + [justice]]\n",
    "  grouped_df = df_j.groupby('case_id')['text'].apply(lambda x: ','.join(x)).reset_index()\n",
    "  justices = df[[\"case_id\"] + [justice]].drop_duplicates(keep='first')\n",
    "\n",
    "  df1 = pd.merge(grouped_df, justices, left_on='case_id', right_on='case_id', how='left').dropna(axis='rows', how='any')\n",
    "  df1 = df1.drop(df1[~df1[justice].isin([0, 1])].index)\n",
    "\n",
    "  justices_dict[justice] = len(df1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorted justices by case load\n",
    "sorted_justices_by_case = sorted(justices_dict.items(), key=lambda x:x[1], reverse=True)\n",
    "sorted_justices_by_case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top justices\n",
    "top_justices = [justice for justice, votes in sorted_justices_by_case[:15]]\n",
    "print(top_justices)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Outcome Predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datasets & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have to do by utterance as grouping by case_id gives too few examples to learn from\n",
    "df_all = df[['win_side', 'text', 'case_id']]\n",
    "df_all.head()\n",
    "\n",
    "# Keep only cases with outcomes 0 or 1\n",
    "df_all = df_all[df_all.win_side.isin([0, 1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique case_ids\n",
    "unique_case_ids = df_all['case_id'].unique()\n",
    "\n",
    "# Split the unique case_ids into training and testing sets\n",
    "train_case_ids, val_test_case_ids = train_test_split(unique_case_ids, test_size=0.2, random_state=123)\n",
    "val_case_ids, test_case_ids = train_test_split(val_test_case_ids, test_size=0.5, random_state=123)\n",
    "\n",
    "# Filter the original dataframe to create the train and test dataframes using the train and test case_ids\n",
    "train_df = df_all[df_all['case_id'].isin(train_case_ids)]\n",
    "val_df = df_all[df_all['case_id'].isin(val_case_ids)]\n",
    "test_df = df_all[df_all['case_id'].isin(test_case_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Words\n",
    "vocab = embed.get_vocab(train_df, min_freq=100)\n",
    "vocab_size = len(vocab)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loaders\n",
    "BATCH_SIZE = 128\n",
    "vocab_size = 300 # Size of GloVe vectors\n",
    "\n",
    "train_dataloader = DataLoader(Dataset.from_pandas(train_df.drop(columns=['case_id']), preserve_index = False), batch_size=BATCH_SIZE,\n",
    "                              shuffle=True,\n",
    "                              collate_fn=lambda batch: embed.collate_into_bow(batch, vocab))\n",
    "valid_dataloader = DataLoader(Dataset.from_pandas(val_df.drop(columns=['case_id']), preserve_index = False), batch_size=BATCH_SIZE,\n",
    "                              shuffle=False, \n",
    "                              collate_fn=lambda batch: embed.collate_into_bow(batch, vocab))\n",
    "test_dataloader = DataLoader(Dataset.from_pandas(test_df.drop(columns=['case_id']), preserve_index = False), batch_size=BATCH_SIZE,\n",
    "                             shuffle=False, \n",
    "                             collate_fn=lambda batch: embed.collate_into_bow(batch, vocab))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BoW NN Classifier\n",
    "hidden_dim = 1000\n",
    "model = pred_models.BoWNNClassifier(vocab_size=vocab_size, hidden_dim=hidden_dim, output_dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 15\n",
    "loss_function = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "best_model = None\n",
    "best_val_loss = float('inf')\n",
    "val_losses = []\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    model_helpers.train_an_epoch(model, train_dataloader, optimizer, loss_function, print_val=True)\n",
    "    val_y_true, val_y_pred = model_helpers.make_predictions(model, valid_dataloader)\n",
    "    val_loss = log_loss(val_y_true.astype(np.float64), val_y_pred.astype(np.float64))\n",
    "    if val_loss < best_val_loss:\n",
    "        best_model = type(model)(model.vocab_size, model.hidden_dim, model.output_dim)\n",
    "        best_model.load_state_dict(model.state_dict())\n",
    "        best_val_loss = val_loss\n",
    "    val_losses.append(val_loss)\n",
    "    time_taken = time.time() - epoch_start_time\n",
    "    print(f'After epoch {epoch} the validation loss is {val_loss:.3f}.')\n",
    "\n",
    "plt.plot(range(1, EPOCHS+1), val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get validation predictions to select best threshold\n",
    "val_labels, val_probs = model_helpers.make_predictions(model, valid_dataloader)\n",
    "# Get best threshold from validation data\n",
    "threshold = model_helpers.select_threshold(val_labels, val_probs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataframe with predictions and real values\n",
    "test_results_df = model_helpers.get_test_results_df(best_model, test_dataloader, test_df[['case_id', 'win_side']])\n",
    "test_results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results dataframe per utterance\n",
    "per_utterance_df = (test_results_df\n",
    "                    .assign(pred=lambda x: x['prob']\n",
    "                    .apply(lambda y: 1 if y > threshold else 0)))\n",
    "\n",
    "# Results dataframe per case\n",
    "per_case_df = (test_results_df\n",
    "                .groupby('case_id')\n",
    "                .mean()\n",
    "                .assign(pred=lambda x: x['prob']\n",
    "                .apply(lambda y: 1 if y > threshold else 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metrics per utterance\n",
    "model_helpers.get_evaluation_matrix(per_utterance_df['win_side'], per_utterance_df['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metrics per case\n",
    "model_helpers.get_evaluation_matrix(per_case_df['win_side'], per_case_df['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix by case\n",
    "model_helpers.results_heatmap(per_case_df['win_side'],\n",
    "                per_case_df['pred'],\n",
    "                'Confusion Matrix by Case',\n",
    "                target_names = ['respondent', 'petitioner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix by utterance\n",
    "model_helpers.results_heatmap(per_utterance_df['win_side'],\n",
    "                per_utterance_df['pred'],\n",
    "                'Confusion Matrix by Utterance',\n",
    "                target_names = ['respondent', 'petitioner'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Justice Outcome Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_justices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters, loss function and optimizer\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 15\n",
    "hidden_dim = 500\n",
    "loss_function = torch.nn.BCELoss()\n",
    "\n",
    "for justice in top_justices:\n",
    "\n",
    "    print(\"Running model for: \", justice)\n",
    "\n",
    "    # Have to do by utterance as grouping by case_id gives too few examples to learn from\n",
    "    df_j = df[[justice, 'text', 'case_id']]\n",
    "    df_j.head()\n",
    "\n",
    "    # Keep only cases with outcomes 0 or 1\n",
    "    df_j = df_j[df_j[justice].isin([0, 1])]\n",
    "\n",
    "    # Get unique case_ids\n",
    "    unique_case_ids = df_j['case_id'].unique()\n",
    "    # Split the unique case_ids into training and testing sets\n",
    "    train_case_ids, val_test_case_ids = train_test_split(unique_case_ids, test_size=0.2, random_state=123)\n",
    "    val_case_ids, test_case_ids = train_test_split(val_test_case_ids, test_size=0.5, random_state=123)\n",
    "\n",
    "    # Filter the original dataframe to create the train and test dataframes using the train and test case_ids\n",
    "    train_df = df_j[df_j['case_id'].isin(train_case_ids)]\n",
    "    val_df = df_j[df_j['case_id'].isin(val_case_ids)]\n",
    "    test_df = df_j[df_j['case_id'].isin(test_case_ids)]\n",
    "\n",
    "    # Data loaders\n",
    "    train_dataloader = DataLoader(Dataset.from_pandas(train_df.drop(columns=['case_id']), preserve_index = False), batch_size=BATCH_SIZE,\n",
    "                                shuffle=True,\n",
    "                                collate_fn=lambda batch: embed.collate_into_bow(batch, vocab, justice))\n",
    "    valid_dataloader = DataLoader(Dataset.from_pandas(val_df.drop(columns=['case_id']), preserve_index = False), batch_size=BATCH_SIZE,\n",
    "                                shuffle=False, \n",
    "                                collate_fn=lambda batch: embed.collate_into_bow(batch, vocab, justice))\n",
    "    test_dataloader = DataLoader(Dataset.from_pandas(test_df.drop(columns=['case_id']), preserve_index = False), batch_size=BATCH_SIZE,\n",
    "                                shuffle=False, \n",
    "                                collate_fn=lambda batch: embed.collate_into_bow(batch, vocab, justice))\n",
    "    \n",
    "    # BoW NN Classifier\n",
    "    model = pred_models.BoWNNClassifier(vocab_size=vocab_size, hidden_dim=hidden_dim, output_dim=1)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    # Train model\n",
    "    print(\"Start training...\")\n",
    "    best_model = None\n",
    "    best_val_loss = float('inf')\n",
    "    val_losses = []\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        model_helpers.train_an_epoch(model, train_dataloader, optimizer, loss_function)\n",
    "        val_y_true, val_y_pred = model_helpers.make_predictions(model, valid_dataloader)\n",
    "        val_loss = log_loss(val_y_true.astype(np.float64), val_y_pred.astype(np.float64))\n",
    "        if val_loss < best_val_loss:\n",
    "            best_model = type(model)(model.vocab_size, model.hidden_dim, model.output_dim)\n",
    "            best_model.load_state_dict(model.state_dict())\n",
    "            best_val_loss = val_loss\n",
    "        val_losses.append(val_loss)\n",
    "        time_taken = time.time() - epoch_start_time\n",
    "        print(f'After epoch {epoch} the validation loss is {val_loss:.3f}.')\n",
    "    \n",
    "    # Get validation predictions to select best threshold\n",
    "    val_labels, val_probs = model_helpers.make_predictions(model, valid_dataloader)\n",
    "    # Get best threshold from validation data\n",
    "    threshold = model_helpers.select_threshold(val_labels, val_probs)\n",
    "\n",
    "    # Model evaluation\n",
    "    print(\"Start test evaluation...\")\n",
    "    # Get dataframe with predictions and real values\n",
    "    test_results_df = model_helpers.get_test_results_df(best_model, test_dataloader, test_df[['case_id', justice]])\n",
    "\n",
    "    # Results dataframe per utterance\n",
    "    per_utterance_df = (test_results_df\n",
    "                        .assign(pred=lambda x: x['prob']\n",
    "                        .apply(lambda y: 1 if y > threshold else 0)))\n",
    "\n",
    "    # Results dataframe per case\n",
    "    per_case_df = (test_results_df\n",
    "                    .groupby('case_id')\n",
    "                    .mean()\n",
    "                    .assign(pred=lambda x: x['prob']\n",
    "                    .apply(lambda y: 1 if y > threshold else 0)))\n",
    "    \n",
    "    # Evaluation metrics per utterance\n",
    "    eval_metrics = model_helpers.get_evaluation_matrix(per_utterance_df[justice], per_utterance_df['pred'])\n",
    "    print(\"Per utterance: \", eval_metrics)\n",
    "\n",
    "    # Evaluation metrics per case\n",
    "    eval_metrics_case = model_helpers.get_evaluation_matrix(per_case_df[justice], per_case_df['pred'])\n",
    "    print(\"Per case: \", eval_metrics_case)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
