{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from convokit import Utterance, Corpus, Coordination, download\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# Set workding directory\n",
    "os.chdir('C:\\\\Users\\\\Jonas\\\\Desktop\\\\UChicago\\\\term_6\\\\AdvancedMachineLearning\\\\supreme_court_nlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_utterances(corpus):\n",
    "    \"\"\"\n",
    "    Cleans utterances by cleaning the text, assessing who is addressed,\n",
    "    dropping some irrelevant columns, and some other miscellaneous\n",
    "    tasks.\n",
    "\n",
    "    Input:\n",
    "        corpus: Corpus object (usually from a given year)\n",
    "\n",
    "    Output:\n",
    "        utterances (pd.DataFrame): clean datafram containing utterances\n",
    "    \"\"\"\n",
    "\n",
    "    # Fetch the utterances\n",
    "    utterances = corpus.get_utterances_dataframe()\n",
    "\n",
    "    # Clean the text\n",
    "    utterances['text'] = utterances['text'].apply(\n",
    "        lambda txt: txt.replace('\\n', ' ')  # Filter such that irrelevant rows are removed (might be irrelevant if pytorch can read \\n)\n",
    "    )\n",
    "\n",
    "    # Drop \"useless\" columns\n",
    "    utterances.drop(\n",
    "        [\n",
    "            'timestamp', 'meta.start_times', 'meta.stop_times', 'vectors'\n",
    "        ],\n",
    "        axis=1,\n",
    "        inplace=True\n",
    "    )\n",
    "\n",
    "    # Rename columns for clarity\n",
    "    SUPERSCRIPT = 'meta.'\n",
    "    colnames_to_transform = [\n",
    "        col[len(SUPERSCRIPT):]\n",
    "        for col in utterances.columns\n",
    "        if col.startswith(SUPERSCRIPT)\n",
    "    ]\n",
    "    utterances.rename(\n",
    "        {\n",
    "            SUPERSCRIPT + col: col\n",
    "            for col in colnames_to_transform\n",
    "        },\n",
    "        axis=1,\n",
    "        inplace=True\n",
    "    )\n",
    "\n",
    "    # Create addressing (\"lag\" of index)\n",
    "    utterances['addressing'] = None\n",
    "    for idx, row in utterances.iterrows():\n",
    "        reply_to = row['reply_to']\n",
    "        if reply_to:\n",
    "            utterances.loc[reply_to]['addressing'] = idx\n",
    "\n",
    "    return utterances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_years(lb_year, ub_year, case_info=True, same_resp_addr=False):\n",
    "    \"\"\"\n",
    "    For a given range of year, the corpus of utterances is\n",
    "    downloaded and cleaned. If desired, information regarding\n",
    "    the cases is added.\n",
    "\n",
    "    Inputs:\n",
    "        - lb_year (int): Lower bound year\n",
    "        - ub_year (int): Upper bound year\n",
    "        - case_info (bool): Wheter case information should\n",
    "            be included\n",
    "        - same_resp_addr (bool): Whether the person responded to\n",
    "            must also be the person addressed\n",
    "            \n",
    "    Returns:\n",
    "        - clean_corpus (pd.DataFrame): The clean dataset\n",
    "    \"\"\"\n",
    "    first = True\n",
    "    for year in range(lb_year, ub_year+1):\n",
    "        # Download the data\n",
    "        ROOT_DIR = download(\n",
    "            f'supreme-{year}',\n",
    "            data_dir=os.getcwd()\n",
    "        )\n",
    "        \n",
    "        # Clean a single year and then concat with previous ones\n",
    "        if first:\n",
    "            clean_corpus = get_clean_utterances(\n",
    "                Corpus(\n",
    "                    ROOT_DIR\n",
    "                )\n",
    "            )\n",
    "            first = False\n",
    "        else:\n",
    "            clean_corpus = pd.concat(\n",
    "                [\n",
    "                    clean_corpus,\n",
    "                    get_clean_utterances(\n",
    "                        Corpus(\n",
    "                            ROOT_DIR\n",
    "                        )\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "        \n",
    "        # Delete the downloaded files\n",
    "        os.remove(f'supreme-{year}.zip')\n",
    "        shutil.rmtree(f'supreme-{year}')\n",
    "    \n",
    "    # Join the case info\n",
    "    if case_info:\n",
    "        ci = pd.read_csv(\n",
    "            'case_info_parsing/case_info_relevant_cols_only.csv',\n",
    "            index_col='id'\n",
    "        )\n",
    "        clean_corpus = clean_corpus.join(\n",
    "            ci,\n",
    "            on='case_id',\n",
    "            how='left'\n",
    "        )\n",
    "\n",
    "    # Add names of those replied and spoken to\n",
    "    clean_corpus = pd.merge(\n",
    "        pd.merge(\n",
    "            clean_corpus,\n",
    "            clean_corpus[['speaker', 'speaker_type']],\n",
    "            left_on='reply_to',\n",
    "            right_index=True,\n",
    "            how='left',\n",
    "            suffixes=('', '_replied_to')\n",
    "        ),\n",
    "        clean_corpus[['speaker', 'speaker_type']],\n",
    "        left_on='addressing',\n",
    "        right_index=True,\n",
    "        how='left',\n",
    "        suffixes=('', '_addressed')\n",
    "    )\n",
    "\n",
    "    # Replied to be the same as the person addressed\n",
    "    if same_resp_addr:\n",
    "        clean_corpus = clean_corpus[\n",
    "            clean_corpus['speaker_replied_to'] == clean_corpus['speaker_addressed']\n",
    "        ]\n",
    "\n",
    "    # Save the resulting datasets in the current directory\n",
    "    clean_corpus.to_csv(f'data/utterances_clean{lb_year}-{ub_year}.csv')\n",
    "    # clean_corpus.to_json(f'utterances_clean{lb_year}-{ub_year}.json')\n",
    "\n",
    "    return clean_corpus\n",
    "\n",
    "# Make sure to have 'case_info_relevant_cols_only.csv' saved\n",
    "# in the current directory prior to running\n",
    "\n",
    "############### VERY IMPORTANT ###############\n",
    "# For the code to work, go to convokit and on line 118 of util.py,\n",
    "# set needs_download = True.\n",
    "# ut = aggregate_years(1975, 1975, case_info=True, same_resp_addr=False)\n",
    "display(ut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_bounds = [\n",
    "    (1955, 1955),\n",
    "    (1956, 1960),\n",
    "    (1961, 1961),\n",
    "    (1993, 1993),\n",
    "    (1994, 1998),\n",
    "    (1999, 1999),\n",
    "    (2013, 2013),\n",
    "    (2014, 2018),\n",
    "    (2019, 2019)\n",
    "]\n",
    "\n",
    "for year_lb, year_ub in year_bounds:\n",
    "    aggregate_years(year_lb, year_ub, case_info=True, same_resp_addr=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
