{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from convokit import Utterance, Corpus, Coordination, download\n",
    "\n",
    "# Set workding directory\n",
    "os.chdir('C:\\\\Users\\\\Jonas\\\\Desktop\\\\UChicago\\\\term_6\\\\AdvancedMachineLearning\\\\project')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_utterances(corpus):\n",
    "    \"\"\"\n",
    "    Cleans utterances by cleaning the text, assessing who is addressed,\n",
    "    dropping some irrelevant columns, and some other miscellaneous\n",
    "    tasks.\n",
    "\n",
    "    Input:\n",
    "        corpus: Corpus object (usually from a given year)\n",
    "\n",
    "    Output:\n",
    "        utterances (pd.DataFrame): clean datafram containing utterances\n",
    "    \"\"\"\n",
    "\n",
    "    # Fetch the utterances\n",
    "    utterances = corpus.get_utterances_dataframe()\n",
    "\n",
    "    # Clean the text\n",
    "    utterances['text'] = utterances['text'].apply(\n",
    "        lambda txt: txt.replace('\\n', ' ')  # Filter such that irrelevant rows are removed (might be irrelevant if pytorch can read \\n)\n",
    "    )\n",
    "\n",
    "    # Drop \"useless\" columns\n",
    "    utterances.drop(\n",
    "        [\n",
    "            'timestamp', 'meta.start_times', 'meta.stop_times', 'vectors'\n",
    "        ],\n",
    "        axis=1,\n",
    "        inplace=True\n",
    "    )\n",
    "\n",
    "    # Rename columns for clarity\n",
    "    SUPERSCRIPT = 'meta.'\n",
    "    colnames_to_transform = [\n",
    "        col[len(SUPERSCRIPT):]\n",
    "        for col in utterances.columns\n",
    "        if col.startswith(SUPERSCRIPT)\n",
    "    ]\n",
    "    utterances.rename(\n",
    "        {\n",
    "            SUPERSCRIPT + col: col\n",
    "            for col in colnames_to_transform\n",
    "        },\n",
    "        axis=1,\n",
    "        inplace=True\n",
    "    )\n",
    "\n",
    "    # Create addressing (\"lag\" of index)\n",
    "    utterances['addressing'] = None\n",
    "    for idx, row in utterances.iterrows():\n",
    "        reply_to = row['reply_to']\n",
    "        if reply_to:\n",
    "            utterances.loc[reply_to]['addressing'] = idx\n",
    "\n",
    "    return utterances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_years(lb_year, ub_year, case_info=True):\n",
    "    \"\"\"\n",
    "    For a given range of year, the corpus of utterances is\n",
    "    downloaded and cleaned. If desired, information regarding\n",
    "    the cases is added.\n",
    "\n",
    "    Inputs:\n",
    "        - lb_year (int): Lower bound year\n",
    "        - ub_year (int): Upper bound year\n",
    "        - case_info (bool): Wheter case information should\n",
    "            be included\n",
    "            \n",
    "    Returns:\n",
    "        - clean_corpus (pd.DataFrame): The clean dataset\n",
    "    \"\"\"\n",
    "    first = True\n",
    "    for year in range(lb_year, ub_year+1):\n",
    "        # Download the data\n",
    "        ROOT_DIR = download(\n",
    "            f'supreme-{year}',\n",
    "            data_dir=os.getcwd()\n",
    "        )\n",
    "        \n",
    "        # Clean a single year and then concat with previous ones\n",
    "        if first:\n",
    "            clean_corpus = get_clean_utterances(\n",
    "                Corpus(\n",
    "                    ROOT_DIR\n",
    "                )\n",
    "            )\n",
    "            first = False\n",
    "        else:\n",
    "            clean_corpus = pd.concat(\n",
    "                [\n",
    "                    clean_corpus,\n",
    "                    get_clean_utterances(\n",
    "                        Corpus(\n",
    "                            ROOT_DIR\n",
    "                        )\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "    \n",
    "    # Join the case info\n",
    "    if case_info:\n",
    "        case_info = pd.read_csv(\n",
    "            'case_info_relevant_cols_only.csv',\n",
    "            index_col='id'\n",
    "        )\n",
    "        clean_corpus.join(\n",
    "            case_info,\n",
    "            on='case_id',\n",
    "            how='left'\n",
    "        )\n",
    "    \n",
    "    # Save the resulting datasets in the current directory\n",
    "    clean_corpus.to_csv(f'utterances_clean{lb_year}-{ub_year}.csv')\n",
    "    clean_corpus.to_json(f'utterances_clean{lb_year}-{ub_year}.json')\n",
    "\n",
    "    return clean_corpus\n",
    "\n",
    "# Make sure to have 'case_info_relevant_cols_only.csv' saved\n",
    "# in the current directory prior to running\n",
    "ut = aggregate_years(2014, 2018, case_info=True)\n",
    "display(ut)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
