{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/michaelwagner/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from torchtext.vocab import GloVe\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# csv loaded up from 'text_cleaning/utterances_flattener.ipynb'\n",
    "df = pd.read_csv('utterances_clean2014-2018.csv')\n",
    "\n",
    "# pre trained model for tokenizing\n",
    "nltk.download('punkt')\n",
    "\n",
    "def get_glove_embeddings(df_column, dim=100):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    df_column: a single column of a data frame of textual data\n",
    "    dim: choose dim of embedding vectors\n",
    "\n",
    "    Outputs:\n",
    "    returns tensor object of embeddings\n",
    "    \"\"\"\n",
    "    glove = GloVe(name='6B', dim=dim)\n",
    "\n",
    "    # set of unique words in the dataframe column, tried this to speed it up, not sure if it worked\n",
    "    # tokenize to clean formatting\n",
    "    unique_words = set(word for sentence in df_column for word in nltk.word_tokenize(sentence))\n",
    "    \n",
    "    # initialize dictionary\n",
    "    word_to_vec = {word: glove[word].numpy() for word in unique_words if word in glove.stoi}\n",
    "\n",
    "    # maps each word to embedding\n",
    "    def map_to_embedding(sentence):\n",
    "        return [word_to_vec.get(word, glove.get_vecs_by_tokens('<unk>').numpy()) \n",
    "                for word in nltk.word_tokenize(sentence)]\n",
    "\n",
    "    # Use a new list to store embeddings instead of modifying the dataframe\n",
    "    # supposedly faster, idk\n",
    "    embeddings_list = df_column.apply(map_to_embedding).tolist()\n",
    "    #convert each list to tensor\n",
    "    embeddings = [torch.tensor(sentence) for sentence in embeddings_list]\n",
    "    # padding\n",
    "    embeddings = torch.nn.utils.rnn.pad_sequence(embeddings, batch_first=True)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "\n",
    "def get_bow_embeddings(df_column, vocab_size=10000):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    df_column: a single column of a data frame of textual data\n",
    "    vocab_size: size of vocabulary to be used\n",
    "\n",
    "    Outputs:\n",
    "    returns tensor object of BoW representations\n",
    "    \"\"\"\n",
    "    # Flatten the dataframe column into a single list of words and count the word frequencies\n",
    "    words = [word for sentence in df_column for word in nltk.word_tokenize(sentence)]\n",
    "    word_freqs = Counter(words)\n",
    "\n",
    "    # Sort the words by frequency and keep only the top vocab_size words\n",
    "    reduced_vocab = {word for word, _ in word_freqs.most_common(vocab_size)}\n",
    "\n",
    "    # Initialize a CountVectorizer object with the reduced vocabulary\n",
    "    vectorizer = CountVectorizer(vocabulary=reduced_vocab)\n",
    "\n",
    "    # Fit the vectorizer on the text data and transform the data\n",
    "    bow = vectorizer.fit_transform(df_column)\n",
    "\n",
    "    # Convert the result to a dense matrix and then to a DataFrame\n",
    "    df_bow = pd.DataFrame(bow.todense(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "    # Convert the DataFrame to a PyTorch tensor\n",
    "    tensor_bow = torch.tensor(df_bow.values)\n",
    "\n",
    "    return tensor_bow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_test = get_glove_embeddings(df['text'], 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([73531, 1915, 100])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 73531 text entries\n",
    "# sentences padded to 1915 (longest sentence)\n",
    "# each word dimension 100\n",
    "glove_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.2252,  0.5219,  0.4974,  ...,  0.0550,  0.4561,  0.6579],\n",
       "        [-0.0378,  0.5388,  0.7541,  ...,  0.1866,  0.4622,  0.3770],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one sentence\n",
    "glove_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1330: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "bow_test = get_bow_embeddings(df['text'], 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([73531, 5000])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 73531 text\n",
    "# 5000 width for vocab\n",
    "bow_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0,  ..., 0, 0, 0])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one sentence\n",
    "bow_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([8, 6, 5, 4, 3, 2, 2, 2, 2, 2]),\n",
       "indices=tensor([4589, 4590, 2719, 2887, 3348, 4143, 1150, 1023, 4221, 2880]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at most frequent words in a text value\n",
    "bow_test[9].topk(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e1998ff7f8aa20ada591c520b972326324e5ea05489af9e422744c7c09f6dad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
